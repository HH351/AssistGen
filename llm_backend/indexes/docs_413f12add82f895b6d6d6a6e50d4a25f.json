{
  "0": {
    "text": "Monad: Towards Cost-effective Specialization for\nChiplet-based Spatial Accelerators\nXiaochen Hao1, Zijian Ding1, Jieming Yin2, Yuan Wang1, Yun Liang1,3*\n1Peking University,2Nanjing University of Posts and Telecommunications\n3Beijing Advanced Innovation Center for Integrated Circuits\nxiaochen.hao@stu.pku.edu.cn, dzj325@gmail.com, jieming.yin@njupt.edu.cn, {wangyuan, ericlyun }@pku.edu.cn\nAbstract —Advanced packaging offers a new design paradigm\nin the post-Moore era, where many small chiplets can be assem-\nbled into a large system. Based on heterogeneous integration, a\nchiplet-based accelerator can be highly specialized for a specific\nworkload, demonstrating extreme efficiency and cost reduction.\nTo fully leverage this potential, it is critical to explore both the\narchitectural design space for individual chiplets and different\nintegration options to assemble these chiplets, which have yet to\nbe fully exploited by existing proposals.\nThis paper proposes Monad, a cost-aware specialization ap-\nproach for chiplet-based spatial accelerators that explores the\ntradeoffs between PPA and fabrication costs. To evaluate a spe-\ncialized system, we introduce a modeling framework considering\nthe non-uniformity in dataflow, pipelining, and communications\nwhen executing multiple tensor workloads on different chiplets.\nWe propose to combine the architecture and integration design\nspace by uniformly encoding the design aspects for both spaces\nand exploring them with a systematic ML-based approach. The\nexperiments demonstrate that Monad can achieve an average of\n16% and 30% EDP reduction compared with the state-of-the-art\nchiplet-based accelerators, Simba and NN-Baton, respectively.\nI. I NTRODUCTION\nThe slowing of Moore’s Law has motivated the industry\nto embrace chiplet-based designs, where a large monolithic\ndie is broken into multiple smaller dies called “chiplets”.\nSmall chiplets benefit from high fabrication yields and short\ndevelopment cycles. By leveraging advanced packaging tech-\nnologies, multiple chiplets can be integrated into the same\npackage, delivering a flexible, high-performance processor at\na reasonable cost [24].\nMeanwhile, as the computing demand for machine learning\nkeeps increasing, spatial accelerators that feature an array of\nprocessing elements (PEs) emerge as an efficient platform and\nalso become larger and larger. For example, Cerebras [3] uses\na wafer-scale accelerator to offer cluster-scale throughput. To\ntake full advantage of the performance, the large accelerators\nare designed to execute a graph of workloads in parallel, e.g.,\nseveral DNN layers rather than a single convolution [2], [33].\nBesides, these accelerators are generally organized in a tiled\narchitecture, where multiple identical small cores are intercon-\nnected with a high-bandwidth network-on-chip (NoC). Due to\ntheir high complexity, these accelerators may experience long\ndevelopment cycles, increased costs, and difficulty adapting to\nspecific applications [6], [38], [39].\n*Corresponding author.Compared to the large monolithic designs, a chiplet-based\naccelerator can offer cost reduction and efficiency benefits by\nusing small dies and allowing packaging-level specialization .\nIn a tiled architecture, the compute and communication re-\nsources are uniformly distributed across the hardware, which\nmay cause resource under-utilization when running a specific\nworkload. The chiplet approach, however, allows designers to\nintegrate specialized small dies into a system to accelerate a\ngraph of workloads. Each chiplet can be designed specifically\nfor one workload with an affordable cost, and the in-package\nnetwork for interconnecting multiple dies can be specialized to\nmatch the communication pattern and bandwidth requirement\nof the target workload graph. In addition, the chiplet and net-\nwork designs can be reused across different accelerators [32],\namortizing their development and fabrication costs.\nSpecializing a chiplet-based accelerator poses challenges in\narchitecting the chiplets and assembling them into a package.\nFrom an architectural perspective, a resource assignment de-\ncides the number of processing elements (PEs) and the amount\nof on-chip buffers in every chiplet. A dataflow describes how\na workload is parallelized and how its data are moved. These\ntwo aspects are critical to acceleration efficiency [16], [20].\nFrom an integration perspective, the in-package wires cannot\nprovide the same bandwidth and/or energy per bit as on-chip\nwires, so that an inefficient in-package network may become a\nperformance bottleneck. Besides, packaging technology plays\na pivotal role since it determines not only available bandwidth\nbut also the fabrication cost. Depending on the performance\nand cost budget requirement, both traditional (but cheap) and\nadvanced (but expensive) packaging should be evaluated.\nWe need a comprehensive chiplet-based spatial accelerator\ndesign methodology to tap their potential. The existing works\nrevolve around either the architectural or the integration as-\npects and follow a separate design flow: given chiplets then\ndesign packaging [1], [36], or given packaging then design\nchiplets [15], [19], [25], [28]. In fact, there is a richer design\nspace if tradeoffs can be made between the architecture and\nintegration design. For example, varying resource assignment\naffects communication demands and subsequently the choice\nof packaging. A separate flow would consider one of them at\na time, leading to a suboptimal design.\nIn this paper, we argue that we should consider PP PA as\nthe design objectives, which stands for p ower, p erformance,\nprice, and a rea. Using cost (price) as a metric can be helpful2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD) | 979-8-3503-2225-5/23/$31.00 ©2023 IEEE | DOI: 10.1109/ICCAD57390.2023.10323880\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 1,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  },
  "1": {
    "text": "Fig. 1: A chiplet-based spatial accelerator with distributed compute\nand memory elements\nin directing optimizations, as chiplet approach can reduce cost\nbut at the expense of the others. Partitioning a monolithic chip\nreduces the cost, but requires adding die-to-die interfaces and\nthus uses extra energy and area. More advanced (and costly)\npackaging technology enhances connectivity, but may reduce\nthe budget on chiplets for computing efficiency.\nWe propose a modeling framework to assess the efficiency\nof specialized chiplets and their interconnection. The chiplets\nvary in resources and dataflow, making it essential to model\nthe parallel hierarchy, computing, data access, and data reuse\nbehaviors. We target expediting a graph of tensor workloads\nby assigning each chiplet a single workload and coordinating\nmultiple chiplets in a pipelined manner. It is critical to model\nthe pipeline efficiency of compute and data transfer stages. In\naddition, the data transfer efficiency may vary with communi-\ncation flows, network structures, and available bandwidth in a\npackaging. Our framework thus models the routing and flow\ncontrol mechanism of a network to capture this variation. The\nmodeling outputs a performance estimation, thereby enabling\nthe exploration of the design space.\nWe further propose to co-design the architecture and inte-\ngration of a chiplet-based accelerator. Specifically, the archi-\ntecture involves resource assignment and dataflow, while the\nintegration involves network and packaging. Our optimization\nframework encodes parameters from the above design aspects\nand automate the exploration with a Bayesian engine.\nIn summary, this paper makes the following contributions:\n•We propose a cost-aware design approach to make com-\nprehensive tradeoffs for a chiplet-based accelerator.\n•We propose a modeling framework to evaluate a chiplet\nsystem with specialized architecture and interconnects.\n•We develop an ML-based co-optimization framework to\ncouple the architecture and integration design space.\nExperiments present an average of 16% and 30% energy-\ndelay-product (EDP) reduction compared with the state-of-the-\nart, Simba [25] and NN-Baton [28], respectively. We achieve\n24% less latency or 16% less energy compared with the best\nof separate architecture or integration optimization.\nII. B ACKGROUND\nA. Hardware and Application Model\nWe model a chiplet-based spatial accelerator in three hier-\narchical levels: a package, chiplets, and cores. As shown in\nFig. 1, a package consists of chiplets, and a chiplet consists\nof multiple cores. A core has a PE array, in which every PE\nhas a MAC unit. The boundary chiplets are connected with\n(a) Organic Substrate\n (b) Silicon Interposer\nFig. 2: Typical advanced packaging technologies\nDRAM, and other chiplets load data through the in-package\ninterconnection. The chiplet buffer is shared by all its cores,\nand the core buffer is shared by all its PEs. Each chiplet can\nbe specialized, i.e., designed for a specific workload, in terms\nof resources and dataflow. PEs, cores, and chiplets are inter-\nconnected. An in-package network (connecting chiplets) can\nbe specialized to match the communication flow of multiple\nchiplets running different workloads.\nWe target tensor workloads such as matrix multiplication,\nconvolution, MTTKRP, etc., which can be described using a\nloop nest, i.e., an operation on tensors. We refer to dataflow\nas the way to orchestrate data movement and data reuse. The\ndataflow can direct where (a PE in the hierarchy) and when (a\nsequence) to execute a loop instance and access its associated\ndata. The same data can be reused across interconnected PEs,\nor across time within the same PE. Modeling dataflow offers\nan accurate estimation of an accelerator’s utilization, latency,\nand energy consumption [16], [20], which facilitates spatial\nhardware synthesis [9], [10], [21].\nB. Advanced Packaging\nAdvanced packaging technologies, including organic sub-\nstrate and passive/active silicon interposer, allow the assem-\nbling of separately manufactured dies in a package. Multiple\ndies can be mounted on an organic substrate in Fig. 2a. A\nsilicon interposer, as given in Fig. 2b, is a die for connectivity\non top of which dies are mounted. It interconnects dies with\nmicrobumps, which provides a high interconnect density (6 ×\ncompared with organic substrate [31]). The active interposer\ncan incorporate active devices, like network routers, to save\nbump resources for higher connectivity [32], [36]. However,\na passive interposer is fabricated with costly processes, while\nan active interposer even relies on standard CMOS processes,\ncausing much-increased costs [27].\nWe calculate the total fabrication cost of chiplet-based ac-\ncelerators by considering costs of die fabrication, die bonding,\nsubstrate and interposer fabrication, and additional processes:\nCtotal=NX\ni=1(Ci\ndie\nyi\ndie+Cbond) +Csub+Cint\nyint+Cproc (1)\nWe consider Ndies to be assembled in a package. Both\ndie cost Ci\ndieand die yield yi\ndiedepend on the die i’s area\nand technology node. The yield is estimated with a negative\nbinomial model. The substrate cost Csuband interposer cost\nCintis proportional to its area, and the interposer yield yint\nis estimated similarly to that of a silicon die. In this work,\nwe focus on standard packaging technologies and fabrication\ncost with an industrial cost model [8], but our method can be\n2\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 2,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  },
  "2": {
    "text": "Fig. 3: Cost comparison for TPU and Gemmini (Gem)\nextended to industrial variants and consider the non-recurring\nengineering (NRE) cost with an advanced model [5].\nWe demonstrate the impact of chiplet scale and packaging\ntechnology on the total fabrication cost by using two typical\nDNN accelerators, TPU [11] and Gemmini [7]. TPU uses a\n256×256 systolic array within 331 mm2area in 28nm node.\nGemmini is assumed to have a 16 ×16 systolic array within\n1.1mm2area in 22nm. Each TPU or Gemmini chip is used as\none chiplet in a system, and we integrate three chiplets with\nan organic substrate or a passive/active interposer. The total\nfabrication cost is estimated with Equation 1. We assume the\nbaseline as a large monolithic die offering the same capability\nas a three-chiplet system, leading to three times the area of a\nsingle chip. We normalize its die cost to 1 for comparison.\nFrom Fig. 3, we can observe that larger chips such as TPU\ndeliver more cost reduction compared to the monolithic die\nbaseline. A smaller chip has a negligible die cost reduction;\ninstead, it requires additional overheads on bonding multiple\nchiplets. Moreover, the interposer is costly–more than 15% of\nthe cost is paid on a passive interposer and more than 30% on\nan active interposer–for both TPU and Gemmini.\nIII. A RCHITECTURE MODELING\nIn this section, we will formulate the modeling problem and\nthen present our framework and performance model.\nA. Problem Formulation\nWe target modeling highly-specialized chiplet-based accel-\nerators. These accelerators are designed to execute a graph of\ntensor workloads, with every workload assigned to a specific\nchiplet. Multiple workloads can be pipelined across different\nchiplets to overlap their processing time. Our framework can\norchestrate multiple chiplets by modeling the non-uniformity\nin dataflow, pipelining, and communication. In the following,\nwe will formulate the mapping and the non-uniformity.\nThe mapping refers to assigning where (a chiplet), when (a\nsequence), and how (a dataflow) to execute workloads:\nDefinition 1: Mapping. A graph G= (V, E )has a set of\nvertices V whose element viis a sub-graph G′, and a set of\nedges E whose element (vi, vj)indicates vjdepends on vi.\nAt the lowest level, G consists of only one vertex to denote\na loop instance. The mapping refers to assigning a vertex vi\nonto a computing engine (PE, core, chip). The mapping has\nto preserve the dependence between vertices.\nFig. 4a gives a Transformer block as an example, which is\ncomposed of 2 heads or 5 matrix multiplies (MMs of vertices0-4). These MMs are mapped to chiplets as shown in Fig. 4d.\nFig. 4b presents a sub-graph in which every vertex is a loop\ninstance of the matrix multiply statement. A dataflow assigns\nthe instances to be executed on cores (each with one PE for\nsimplicity). This dataflow parallelizes the output matrix (loop\niandj); thus, input data (e.g., A[0,0] ) can be reused and\ntransferred from core 0 to core 1. Another chiplet may adopt\na different dataflow or target a different workload. This non-\nuniform dataflow requires modeling various dataflow and the\ncommunication between diverse chiplets.\nIn Fig. 4b, the time step t=0-3 denotes the pipeline stages\nwhere each core (PE) computes a loop instance and transfers\nthe reuse value. However, this approach incurs frequent syn-\nchronizations and communications between the asynchronous\ncores, leading to additional overhead and under-utilization of\nthe network bandwidth. To address this issue, we propose to\ndefine a hierarchical dataflow as illustrated in Fig. 4c, where\nthe instances are processed locally (local time lt=0,1 ) and\nthen the reuse values ( A[0,0] andA[0,1] ) are transferred\nat each global time ( gt=0 ) collectively. This approach intro-\nduces a sub-graph level, with every vertex denoting the local\ninstances. Generally, a graph hierarchy represents a hardware\nhierarchy; a high-level graph consists of vertices assigned for\nchiplets, and low-level graphs for cores and PEs. Hence, the\npipeline structure consists of multiple hierarchical levels, with\neach level composed of its own set of pipelining stages. This\nnon-uniform pipelining requires representing a hierarchy and\nmodeling the possible pipeline stalls.\nCommunication in a system can be represented as a graph,\nwith each chiplet serving as a network node. DRAM access is\ntreated as a specialized form of communication that originates\nor terminates at a memory controller. This graph is created to\nanalyze data transfer efficiency by considering the intercon-\nnection and available bandwidth provided with an integration\ndesign (in-package network and packaging).\nDefinition 2: Communication Graph. A graph Gc=\n(N, L)with each vertex ni∈Nbeing a network node and\neach edge li,j∈Lbeing the communication flow from nito\nnj, whose weight bwri,jdenotes its bandwidth requirement.\nIn-package interconnects provide limited bandwidth com-\npared to on-chip ones, exhibiting higher latency and possible\nperformance degradation. Every communication flow (includ-\ning DRAM access) may experience varying levels of latency\ndue to differences in routing path and resource contention on\nshared network links. This non-uniform communication can\nbe captured by a contention-aware network model.\nThe existing frameworks [13], [28], [38] generally model\neach chip (core) separately and cannot address the three-level\nnon-uniformity. We propose a hierarchical dataflow represen-\ntation to handle it. Note we target modeling a single execution\nof the accelerator; only the final results are written to DRAM.\nB. Mapping framework\nWe first map each workload onto computing engines (PEs,\ncores, chips) and then analyze their dependencies. We define\nthe computing engine domain for each workload as a cluster.\n3\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 3,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  },
  "3": {
    "text": "(a) Dependency graph\n (b) Sub-graph of loop instances\n (c) Sub-graph of pipelining stages\n (d) A mapping strategy\nFig. 4: The mapping formulation. The red arrows represent data transfers, and the dashed blue lines represent pipelining stages.\nDefinition 3: Cluster. A cluster is a domain of computing\nengines where the loop instances of a workload are mapped.\nFor example, the domain of chiplets in Fig. 1 is Chip [i, j] :\n0≤i, j≤1. The domain of cores and PEs can be defined\nsimilarly.\nEach workload has its cluster; no workload is mapped to\nmultiple clusters, and no cluster is shared among workloads.\nOur mapping framework contains three operations. First, we\nassociate a loop instance with a coordinate inside a cluster:\nMap (G, χ) ={v→χ[⃗ p0, ⃗ p1, ⃗ p2]}, v∈G.V\nwhere G is a dependency graph with vertex vbeing a loop\ninstance. χis a cluster, whose element ⃗ p0−2is the coordinate\nof chiplets, cores, and PEs, respectively. This approach gives\nthe flexibility to describe different parallelization options and\ndataflows. For example, in the case of matrix multiplication,\nwe can parallelize the output matrix with the mapping opera-\ntionS[i, j, k ]→PE[i%2, j%2] or parallelize the input matrix\nwithS[i, j, k ]→PE[i%2, k%2] on a 2×2PE array.\nSecond, we associate a chiplet’s coordinate inside a cluster\nwith a chiplet in a system:\nBind (χ, C) ={χ[⃗ p0]→C[⃗ p1]}\nwhere Cis the domain of chiplets in a system, so it binds a\ncluster’s chiplet ⃗ p0to system’s chiplet ⃗ p1. The binding order\nindicates the execution sequence. For example, in Fig. 4d, we\nfirst bind workload 0 onto chiplet 0 then workload 2 onto the\nsame chiplet so that the two workloads are executed sequen-\ntially. A cluster also provides logical chiplet interconnects to\nanalyze the data reuse. For example, we can bind workloads\nonChip [0,0]andChip [1,1]in Fig. 1, and these chiplets can\nbe considered directly connected, ignoring the actual network\ndesign to build a communication graph. Then from the graph,\nwe derive and analyze the real communication flows.\nThird, we generate a hierarchical graph by reducing some\nvertices into a new vertex under a specified rule:\nReduce r(G, G′) ={v→v′}, v∈G.V, v′∈G′.V\nIt reduces multiple vertices vin a graph Ginto a new vertex\nv′to generate a new graph G′under a rule r. The rule can\nbe set to gather instances mapped to each core, as shown in\nFig. 4c. We can interleave the map andreduce operations toenable a hierarchical mapping method, i.e., assign a PE-level\ndataflow ( map), gather loop instances ( reduce ), then assign a\ncore-level dataflow ( map), and so on. Finally, we can bind the\ndataflow to chiplets where a workload will be executed. The\noperations are performed for each workload in a dependency\ngraph, which directs how the graph is mapped.\nWe find that two dependent workloads can be pipelined if\nthey are mapped to different chiplets. To make a synchronized\nexecution of multiple workloads, we apply a reduce operation\non the chiplet-level graph, with the rule gathering instances of\na pipeline stage. For example, a batch of feature maps can be\npipelined between chiplets processing different DNN layers.\nA reduced vertex contains computations for a batch and can\nbe assigned with a dataflow. However, a workload dependency\ngraph cannot present data dependencies. For example, work-\nload 4 is mapped to chiplet 2-3 in Fig. 4d, so we cannot find\nto which chiplet the workload 2 and 3’s result should be sent\n(both chiplets are the possible destinations). We can examine\nthe access pattern of a tensor to build data dependency.\nIntuitively, if an element is accessed many times, only the\nfirst read needs external data to initialize the operation, and\nonly the last write indicates the operations are completed. We\ndenote the set of vertices in Gthat access an element F[⃗f]as\nPG,F[⃗f], which is sorted in its execution order. A set of edges\nΩrepresents data dependence between two workloads:\nΩG1,G2={(max PG1,F[⃗f],minPG2,F[⃗f])| ∀⃗f}\nwhere G1andG2are a producer and consumer, respectively\n(the connected vertices in a dependence graph), and Fis the\ndependent tensor. This equation examines each element in F,\nand connects the last instance (i.e., max) inG1and the first\ninstance ( min) inG2that access the same element, indicating\ntwo communicating chiplets.\nC. Performance modeling\nOur performance model can take a mapping description as\ninput and calculates different metrics by considering pipeline\nefficiency. The workloads’ execution can be pipelined (named\nascomputing stages ), and data transfer can be pipelined with\ncomputing (as data transfer stages ). The pipeline stages can be\nderived by traversing the outermost-level graph. For example,\nwe can assume the whole MMs are pipelined in Fig. 4d. The\n4\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 4,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  },
  "4": {
    "text": "(a) Pipelining diagram\n (b) Data transfer diagram\nFig. 5: Execution diagrams. e0,1is data transfer from chiplet 0 to 2 and e1,2\nis data transfer from chiplet 1 to 2. With a linear topology and a fair allocation\npolicy, only half of the link bandwidth can be occupied by e0,1.\nworkloads mapped to the same chiplet (e.g., MMs 0 and 2) are\nmodeled as a long stage to be pipelined with others. The data\ntransfer stages are added between the two computing stages.\nThe derived pipelining time diagram is shown in Fig. 5a.\nWe can estimate both latency and throughput. The latency\nrefers to the delay to produce an outcome, while the through-\nput refers to the number of outcomes in a period. Specifically,\nwe can define a path interleaving stages p= (vi, ei,j, vj, ...),\nwhere vi, vjare computing stages and ei,jis a data transfer\nstage, as shown in Fig. 5a:\nLat= max\np∈P{X\nv∈pD(v)}, Thr =1\nmax v∈V{D(v)}\nwhere P is the set of paths and D(v)is the delay of a stage v.\nThe latency is the maximal sum of delays on a path, and the\nthroughput is the reciprocal of the longest delay of all stages\n(Vis the set of all stages).\nThe delay of a computing stage is modeled hierarchically;\nDL\nC=|VL|\nULN×max{DLLi\nC, DLLi\nB, DLLi\nA}, i∈[0, N]\nwhere Lis a level (core or chiplet) with Nlower-level ( LL)\nengines, |VL|is the number of vertices to be mapped, and\nULis the average utilization of these engines. Every vertex’s\nexecution consumes the local processing delay, which is the\nmaximum between computing ( DC), memory access ( DB),\nand data transfer ( DA) delays. These values can be obtained\nfrom a data reuse analysis framework [16], [20].\nThe delay of a data transfer stage ei,jis preferably bounded\nby the delay of its associated computing stages. We thus derive\nthe bandwidth requirement of it as bwri,j=|ΩGi,Gj|\nmin{D(vi),D(vj)}\n(|ΩGi,Gj|is data transfer volume), and add it into the commu-\nnication graph. From the graph, our network model estimates\ndelay in given topology, routing, and flow control. The routing\nalgorithm is assumed to be deterministic; it always picks the\nsame path between any two nodes. If multiple flows compete\nfor a link (e.g., e0,1ande1,2compete for the link from chip\n1 to 2 in Fig. 5b), a flow control mechanism can throttle data\nrate to manage resources. We can allocate bandwidth among\nflows in proportion to their requirements if the sum of them\nexceeds the available bandwidth. The data transfer delay adds\nthe switch and serialization delay, respectively:\nD(ei,j) =max f∈F{|f| ·ts+|ΩGi,Gj|\nmin{ebwf\nc}}, c∈fwhere Fis the set of communication flows associated with\nei,j.|f|is the hop count and tsis the delay through one\nrouter. As shown in Fig. 5b, the head flit suffers such a delay,\nbut the body flits are pipelined. ebwf\ncis channel c’s effective\nbandwidth for flow f(e.g., ebwf\n1,2=1\n2bw1,2in Fig. 5b), and\nthe minimum throttles a flow.\nIV. O PTIMIZATION FRAMEWORK\nThe proposed co-optimization framework can encode archi-\ntecture and integration design parameters, and optimize both\nwith an ML-based exploration engine.\nA. Co-optimization Flow\nOur framework is designed to explore an accelerator exe-\ncuting multiple tensor workloads in a single run. Optimizing\na whole application, like a CNN model, might require parti-\ntioning the application into multiple runs with variable input\nshapes and co-optimizing those runs to maximize the end-to-\nend performance, which is not the focus of our work.\nThe proposed co-optimization flow consists of two stages,\ni.e., architecture and integration stages, as shown in Fig. 6a.\nThe architecture stage explores chiplet designs and keeps the\npareto optimal ones. The co-design is enabled by encoding the\navailable designs in the integration stage. We then explore the\nintegration-related design aspects. These stages are equipped\nwith an ML-based optimization engine for high sample effi-\nciency. The optimization can make tradeoffs between perfor-\nmance, energy, cost, and area. It evaluates each sampled point\nusing our models. The energy or area is estimated by adding\noverheads on MACs, memories, and networks.\nB. Encoding Scheme\nThe architectural optimizer accepts a workload description\n(the dependency, statement, and tensor shapes) and explores\nchiplet designs where the given workloads are mapped. The\nencoding scheme we used is illustrated in Fig. 6a:\n•Shape. The geometry of PE, core, and chiplet array.\n•Spatial. Loops that are assigned for spatial paralleliza-\ntion. Every iteration of these loops is parallelized across\na level of computing engines.\n•Order. The permutation of loops indicates the execution\norder of these loops.\n•Tiling. The tile size, e.g., [I0, J0, K0]for loops i0, j0, k0\nindicates a tile A[I0, K0], B[K0, J0]to be processed in\nevery chiplet.\nThe shape andtiling fields correspond to resource assign-\nment, and the spatial andorder fields correspond to dataflow.\nWe set the buffer size for each tensor based on the tile size.\nEach level (chiplets, cores, PEs) has its individual parameters,\nand can be translated into a mapping operation for modeling.\nThe encoding strategy is designed for a single workload, but\nmultiple workloads will be jointly optimized. We can specify\nthe loop (e.g., a batch), inside of which would be a pipeline\nstage. The buffer size is altered accordingly to accommodate\nthe intermediate data transferred through interconnection. In\naddition, the throughput of pipelined workloads is limited by\n5\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 5,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  },
  "5": {
    "text": "(a) Optimization flow with two stages and our encoding schemes\n (b) Bayesian algorithm\nFig. 6: The illustration of our architecture and integration co-optimization framework\nthe one with maximum computations. To improve efficiency,\nwe can assume a total number of PEs and assign PEs to each\nworkload (chiplet) based on the amount of computations, so\nthat the time spent on each stage is roughly balanced.\nThe integration optimizer explores different packaging de-\nsigns. The encoding scheme we used is illustrated in Fig. 6a.\nWe assign a unique number to the design alternatives ( design\nid) as well as the chiplets in the selected design ( chiplet id ).\n•Packaging. The available options, organic substrate, pas-\nsive and active interposer, are encoded as 0-2.\n•Network. Each network design encodes an identifier that\nrepresents topology and routing.\n•Design Selector. Each workload encodes a design id .\n•Placement. Each network node encodes a chiplet id .\nIn Fig. 6a, the organic substrate packaging and mesh net-\nwork are chosen. The 2nd and 5th designs are chosen (in the\ndesign selector field) for integration, and chiplets in the two\ndesigns are numbered; chiplets 0-1 are from the 2nd design,\nand 2-3 are from the 5th design. The network nodes are also\nnumbered along columns then rows, and the placement field\n[0,2,3,1]indicates that these chiplets are placed to node 0-3,\nrespectively. Some cases may be skipped in exploration, e.g.,\nthe total number of chiplets exceeds that of network nodes.\nAfter decoding the information from both architecture and\nintegration stages, we can analyze the communication pattern\nin a system (as presented in Fig. 6a) and evaluate the system-\nlevel performance in terms of energy, cost, and area. We set\nnetwork bandwidth based on the specific requirement of each\ncommunication flow. Specifically, the bandwidth is given by\nsumming up the requirements on every network channel and\nset as the highest requirement among all the channels (i.e., a\nhotspot). We then adjust the allocated I/O resources, leading\nto significant variations in area and cost among designs.\nIt reserves more area for data I/O in a chiplet to support\nhigher bandwidth. Bandwidth density Dbwoffers the allowed\nbandwidth per area ( GB/s/mm2), varying with packaging\nand die-to-die interface [31]. The reserved area is estimated\nasbw\nDbw×Nlink, in which bwis the bandwidth, and Nlink\nis the number of links through bumps. For organic substrate\nand passive interposer, routers are inside chiplets, so all thelinks pass through bumps. For active interposer, routers are\ninside the interposer, so only two links connected to a chiplet\npass through bumps. The reserved area cannot be scaled with\nadvanced technology and thus hinders cost reduction.\nC. Optimization Engine\nOur encoding strategy exhibits a multi-dimensional design\nspace that poses challenges for optimization. In our uniform\nencoding, we observe that some fields have fixed, relatively-\nlow dimensionalities while others may have high dimension-\nalities. In the architecture stage, fields shape andspatial are\nalways two dimensions, while order andtiling are dependent\non the specific workloads. For example, a convolution has 7\nloops and 42 dimensions in total for both fields across three\nlevels. In the integration stage, field packaging andnetwork\nare identifiers, design selector is small with a few workloads,\nandplacement depends on network nodes (up to 36).\nFig. 6b illustrates our optimization engine that combines a\nBayesian (Bayes) and a simulated annealing (SA) algorithm.\nBayes exhibits significant efficiency in optimizing an expen-\nsive black-box function. In our design, Bayes samples points\nfor the low-dimensional fields, and the function is evaluated\nby executing an SA engine to optimize the high-dimensional\nfields and estimating performance. It finds the global optimal\nin a few attempts by adding prior information to a surrogate\nmodel and picking a point at every step with an acquisition\nfunction. We take a Gaussian Process as the surrogate model\nand a probability of improvement as the acquisition function,\nwhich is widely used for its high efficiency [35]. SA exhibits\nmore randomness and suits for high-dimensional fields.\nV. E VALUATION\nA. Experimental Setup and Validation\nWe utilize TENET [20] to analyze data reuse, and Accel-\nergy [34] to estimate area and energy. The fabrication cost is\nfrom ICKnowledge [8] under 28nm. The die-to-die interface\nis UCIe [31]. We validate the proposed modeling framework\nagainst ScaleSim [23], a systolic array simulator. We assume\na four-chip accelerator running Transformer, where each chip\n6\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 6,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  },
  "6": {
    "text": "Fig. 7: The optimization comparison. We label each group with a prefix (S\nfor Simba [25], B for NN-Baton [28], and M for Monad) followed by the\nworkload: res[2-5] are from ResNet-50, and att[1-4] are from BERT-large.\nhas an 8 ×8 PE array. The latency estimated by our modeling\nis within 9.8% error against the simulation results.\nB. Comparison\nWe compare Monad with the state-of-the-art chiplet-based\nDNN accelerators, i.e., Simba [25] and NN-Baton [28], by\nrealizing their hardware configurations (the same number of\nPEs and die-to-die interfaces) and mapping strategies in our\nframework. We collect workloads from typical DNN models,\nres[2-5]b branch2b convolution layers from Resnet-50, and\nfour matrix multiply shapes from BERT-large. The parameters\nare searched with our optimizer.\nWe adopt energy-delay product (EDP) as our optimization\nobjective. The comparison of energy and latency, along with\nthe breakdown of energy, is shown in Fig. 7. The results are\nnormalized to that of Simba in every setting. We achieve an\naverage of 16% and30% EDP reduction compared to Simba\nand NN-Baton, respectively.\nWe achieve an average of 8% and 20.8% energy reduction\ncompared with Simba [25] and NN-Baton [28], respectively.\nNN-Baton uses a ring network where data are rotated among\nchiplets. It chooses the input with less volume to be reused.\nFor example, in activation-intensive layers (with large feature\nmaps), it reuses weights among chiplets. This strategy gains\nimprovements in settings with unbalanced data volume, such\nasres2, to transfer fewer data and exhibit lower energy than\nSimba. However, when both inputs are large, as seen in res3,\nthe die-to-die data transfer overhead is obtrusive. For latency\ncomparison, Simba maps loop instances by dividing input and\noutput channels. As a result, it may suffer a long latency on\nthe earlier layer (e.g., res2) due to inadequate parallelism on\nchannels. On the other hand, NN-Baton chooses a mapping\nstrategy that divides an output plane, making it less effective\nin cases with inadequate parallelism on outputs.\nIn summary, each workload has its own optimal microar-\nchitecture design. We can specialize the design for a specific\nworkload to gain improvement, and our modeling framework\nis general to encompass various design options.\nC. Design Space Analysis\nOur proposed co-design approach demonstrates an entan-\ngled design space. To illustrate the co-design efficiency, we\nseparately enable different optimizations and their combina-\ntions and subsequently report the improvements achieved by\neach specific setting. Specifically, we evaluate a Transformer\nblock by mapping each matrix multiply workload to chiplets\n(a) High performance\n (b) Energy efficiency\nFig. 8: The co-design space analysis. We label each setting as follows: Rand -\nRandom, Res-Resource, Dfw-Dataflow, Arch -Architecture, Net-Network, Pkg-\nPackaging, Inte-Integration, Co-opt -Co-optimization.\nFig. 9: Sampled design points and a Pareto optimal analysis for the\ntradeoffs between cost and latency.\nand exploring designs optimized for performance and energy\nefficiency, respectively. To establish the baseline, we assume\narandom setting using a Simba-like hardware configuration\nwith random parameters. The relative improvements achieved\nby enabling different optimizations are reported in Fig. 8. We\nachieve 24% less latency or 16% less energy compared to the\nbest of separate architecture or integration optimization.\nWe consider end-to-end latency as the performance metric.\nAs shown in Fig. 8a, the architecture optimization ( 4thbar),\nrealized by combining resource assignment and dataflow, can\nresult in 6.1×latency reduction by exploiting more PEs and\nimproved utilization. The integration optimization ( 7thbar),\nachieved by combining network and packaging, leads to 1.3×\nlatency reduction. Our bandwidth setting varies depending on\nthe specific hotspot, and as discussed in §III-C, data transfer\ndelays contribute to the overall latency. Varying network and\npackaging can deliver a new hotspot and affect data transfer\ndelays. Overall, co-optimizing computing and communication\nshows a remarkable 8.1×latency reduction ( 8thbar).\nThe results for energy efficiency are illustrated in Fig. 8b.\nThe architecture optimization achieves 3.2×energy reduction\nby exploring tradeoffs between two interrelated aspects; allo-\ncating a larger buffer increases energy consumption, but also\nfacilitates dataflow exploration to gain benefits from on-chip\ndata reuse. The integration optimization leads to 1.2×energy\nreduction by reducing the number of hops in data forwarding,\nthereby minimizing energy consumption associated with die-\nto-die interfaces. This can be achieved by changing network\ntopology and placement ( 5thbar), and by varying the chiplets\nto be integrated ( 6th bar ). Overall, the co-optimization leads\nto3.9×energy reduction by enabling more tradeoffs.\nA cost-effective design approach aims at minimizing cost\n7\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 7,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  },
  "7": {
    "text": "(a) A tensor network for TT\n (b) A chiplet-based accelerator\nFig. 10: Case study of tensor-train (TT) decomposition. It is represented as\na tensor network, where tensors are denoted as rectangles and tensor indices\nare denoted as lines ( s1−5,a1−4). The line connecting two tensors implies\ntensor contraction Cx,y(x, y are the number of indices of the two tensors,\nrespectively) over the connected indices ( a1−4) [30]. After that, we will get\na new tensor with x+y−2dimensions.\nwhile keeping the same level of performance. We still adopt a\nTransformer block but optimize for cost-effectiveness. Some\nsampled design points classified in their packaging technolo-\ngies are shown in Fig. 9. There is a wide range of costs (up\nto7×) in the same level of latency, which indicates varying\nlevels of resource utilization and bandwidth requirements. A\ncostly interposer can reduce the overall cost by allocating less\narea for I/O. Therefore, cost optimization necessitates striking\na balance between computing and communication efficiency,\nentailing a comprehensive co-design flow.\nD. Case study\nCost is a crucial objective when exploring the design space\nof chiplet-based systems. Ignoring cost often leads to a large\nmonolithic design and limited exploration of communication\nefficiency in advanced packaging. We illustrate these design\nconsiderations through a concrete example.\nTensor train (TT) decomposition provides a compact repre-\nsentation for high-dimensional tensors and finds applications\nin quantum physics and machine learning [26]. As shown in\nFig. 10a, a 5-dimensional tensor Tcan be decomposed into a\nsequence of smaller tensors A. The first and last ones, As1a1\nandAs5a4, are matrices, and the others are 3D tensors. These\ntensors are contracted by performing inner products over the\nshared indices, i.e., a generalized matrix multiply for tensors.\nFor example, we can get a new tensor As1s2a2by contracting\nAs1a1andAs2a1a2overa1. After one-by-one contraction, the\nresult tensor gradually expands in dimensions until 5.\nFig. 10b presents our accelerator design, which adopts one\nchiplet for the lower-dimensional workloads ( C2,3andC3,3)\nand two chiplets for the higher-dimensional workloads ( C4,3\nandC5,2withO(n6)computations). Using two chiplets can\nbring 28% cost reduction compared to the monolithic design.\nWe observe that when the cost is not taken into account, our\noptimizer may scale up a chip to avoid additional overheads.\nThough we can set an area constraint based on potential cost\nreduction, directly using cost as an objective would consider\nthe fabrication node, yield, and intricate interplay with other\ncomponents. For instance, we still employ a single chip for\nC3,3despite its larger size ( >300mm2). Solely focusing on\narea cannot make optimal cost reduction strategies.To match the communication requirement of the sequential\ntensor contraction workloads, a ring network is chosen with a\npassive interposer. Trading cost and performance is critical in\ndetermining whether the enhanced connectivity can amortize\nthe cost of advanced packaging. Besides, we observe that the\nenergy consumption decreases with advances in packaging; it\nbecomes lower than that of SRAM (0.25 pJ/bit [31] vs. 0.81\npJ/bit [28]). Thus, solely focusing on energy may result in a\ndesign that excessively relies on network data transfers with\nimpractical bandwidth requirements. Considering cost makes\na direct influence on communication efficiency. For example,\nthe optimized design in Fig. 10b assigns a dedicated channel\nfor each flow to avoid extra costs on I/O resources.\nVI. R ELATED WORK\nMulti-chip accelerators. There has been significant work\nfocusing on the tiled multi-core architecture. TANGRAM [6]\nproposes dataflow optimization for intra-layer parallelism and\ninter-layer pipelining. Atomic [38] proposes an optimization\nframework to map and schedule a DNN model on a scalable\naccelerator. Simba [25] and NN-Baton [28] introduce MCM-\nbased DNN accelerators and workload mapping methods for\nthem. SPRINT [19] develops a chiplet-based accelerator with\nphotonic interconnects. Some work instead targets specialized\ncores. Herald [17] and MAGMA [13] schedule DNN layers\nonto the cores with different dataflows for efficiency. Krish-\nnan et al [15] exhibits an in-memory computing architecture\nwith big-little chiplets to improve utilization. H2H [37] and\nCOMB [40] propose communication-aware mapping frame-\nworks for heterogeneous systems. However, they commonly\nconsider each chip would run independently and thus model\neach chip separately, with less focus on their coordination.\nChiplet. A large body of work focuses on interposer-based\nnetwork designs [1], [18], [32], [36]. Several works [12], [14]\nintroduce EDA flows that couple architecture and packaging\ndesign. Coskun et al. [4] jointly explore the network topology\nand chiplet placement. Pal et al. [22] select suitable chiplets\nfrom many candidates to build an efficient processor, but they\nmiss the integration design space. Feng et al. [5] and Tang et\nal. [29] suggest to explore the chiplet granularity, heterogene-\nity, and reuse strategies using a cost model. In contrast, this\npaper makes a tradeoff between PPA and fabrication cost by\nintroducing a performance model.\nVII. C ONCLUSION\nThis paper proposed Monad, an exploration framework for\nchiplet-based spatial accelerators. We developed an architec-\nture modeling framework to evaluate performance considering\nthe non-uniformities of specialized chiplets. We also proposed\nto couple the architectural and integration design spaces with\nan ML-based optimization approach. The experiment demon-\nstrated a significant EDP reduction and a huge tradeoff space\nby incorporating the cost as a design objective.\nACKNOWLEDGMENT\nThis work is supported in part by the National Natural Sci-\nence Foundation of China (NSFC) under grant No.U21B2017.\n8\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 8,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  },
  "8": {
    "text": "REFERENCES\n[1] S. Bharadwaj, J. Yin, B. Beckmann, and T. Krishna, “Kite: A family of\nheterogeneous interposer topologies enabled via accurate interconnect\nmodeling,” in 57th ACM/IEEE Design Automation Conference (DAC) ,\n2020.\n[2] X. Cai, Y . Wang, X. Ma, Y . Han, and L. Zhang, “Deepburning-seg:\nGenerating dnn accelerators of segment-grained pipeline architecture,”\nin55th IEEE/ACM International Symposium on Microarchitecture (MI-\nCRO) , 2022.\n[3] Cerebras, “The Future of AI is Here,” https://cerebras.net/chip/.\n[4] A. Coskun, F. Eris, A. Joshi, A. B. Kahng, Y . Ma, A. Narayan, and\nV . Srinivas, “Cross-layer co-optimization of network design and chiplet\nplacement in 2.5-d systems,” IEEE Transactions on Computer-Aided\nDesign of Integrated Circuits and Systems (TCAD) , 2020.\n[5] Y . Feng and K. Ma, “Chiplet actuary: a quantitative cost model and\nmulti-chiplet architecture exploration,” in 59th ACM/IEEE Design Au-\ntomation Conference (DAC) , 2022.\n[6] M. Gao, X. Yang, J. Pu, M. Horowitz, and C. Kozyrakis, “Tangram:\nOptimized coarse-grained dataflow for scalable nn accelerators,” in 24th\nACM International Conference on Architectural Support for Program-\nming Languages and Operating Systems (ASPLOS) , 2019.\n[7] H. Genc, S. Kim, A. Amid, A. Haj-Ali, V . Iyer, P. Prakash, J. Zhao,\nD. Grubb, H. Liew, H. Mao et al. , “Gemmini: Enabling systematic\ndeep-learning architecture evaluation via full-stack integration,” in 58th\nACM/IEEE Design Automation Conference (DAC) , 2021.\n[8] ICKnowledge, “IC Knowledge cost models,” https://www.icknowledge.\ncom/products/costmodels.html.\n[9] L. Jia, Z. Luo, L. Lu, and Y . Liang, “Tensorlib: A spatial accelerator\ngeneration framework for tensor algebra,” in 58th ACM/IEEE Design\nAutomation Conference (DAC) , 2021.\n[10] L. Jia, Y . Wang, J. Leng, and Y . Liang, “EMS: efficient memory\nsubsystem synthesis for spatial accelerators,” in 59th ACM/IEEE Design\nAutomation Conference (DAC) , 2022.\n[11] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,\nS. Bates, S. Bhatia, N. Boden, A. Borchers et al. , “In-datacenter\nperformance analysis of a tensor processing unit,” in 44th ACM/IEEE\nInternational Symposium on Computer Architecture (ISCA) , 2017.\n[12] M. A. Kabir, D. Petranovic, and Y . Peng, “Coupling extraction and\noptimization for heterogeneous 2.5d chiplet-package co-design,” in\n39th ACM/IEEE International Conference on Computer-Aided Design\n(ICCAD) , 2020.\n[13] S.-C. Kao and T. Krishna, “Magma: An optimization framework for\nmapping multiple dnns on multiple accelerator cores,” in 28th IEEE\nInternational Symposium on High-Performance Computer Architecture\n(HPCA) , 2022.\n[14] J. Kim, G. Murali, H. Park, E. Qin, H. Kwon, V . Chaitanya, K. Chekuri,\nN. Dasari, A. Singh, M. Lee et al. , “Architecture, chip, and package co-\ndesign flow for 2.5d ic design enabling heterogeneous ip reuse,” in 56th\nACM/IEEE Design Automation Conference (DAC) , 2019.\n[15] G. Krishnan, A. A. Goksoy, S. K. Mandal, Z. Wang, C. Chakrabarti, J.-s.\nSeo, U. Y . Ogras, and Y . Cao, “Big-little chiplets for in-memory acceler-\nation of dnns: A scalable heterogeneous architecture,” in 41st ACM/IEEE\nInternational Conference on Computer-Aided Design (ICCAD) , 2022.\n[16] H. Kwon, P. Chatarasi, M. Pellauer, A. Parashar, V . Sarkar, and T. Kr-\nishna, “Understanding reuse, performance, and hardware cost of dnn\ndataflow: A data-centric approach,” in 52nd IEEE/ACM International\nSymposium on Microarchitecture (MICRO) , 2019.\n[17] H. Kwon, L. Lai, M. Pellauer, T. Krishna, Y .-H. Chen, and V . Chandra,\n“Heterogeneous dataflow accelerators for multi-dnn workloads,” in 27th\nIEEE International Symposium on High-Performance Computer Archi-\ntecture (HPCA) , 2021.\n[18] F. Li, Y . Wang, Y . Cheng, Y . Wang, Y . Han, H. Li, and X. Li, “Gia: A\nreusable general interposer architecture for agile chiplet integration,” in\n41st IEEE/ACM International Conference on Computer-Aided Design\n(ICCAD) , 2022.\n[19] Y . Li, A. Louri, and A. Karanth, “Scaling deep-learning inference\nwith chiplet-based architecture and photonic interconnects,” in 58th\nACM/IEEE Design Automation Conference (DAC) , 2021.\n[20] L. Lu, N. Guan, Y . Wang, L. Jia, Z. Luo, J. Yin, J. Cong, and\nY . Liang, “Tenet: A framework for modeling tensor dataflow based on\nrelation-centric notation,” in 48th ACM/IEEE International Symposium\non Computer Architecture (ISCA) , 2021.[21] Z. Luo, L. Lu, S. Zheng, J. Yin, J. Cong, J. Yin, and Y . Liang,\n“Rubick: A synthesis framework for spatial architectures via dataflow\ndecomposition,” in 60th ACM/IEEE Design Automation Conference\n(DAC) , 2023.\n[22] S. Pal, D. Petrisko, R. Kumar, and P. Gupta, “Design space exploration\nfor chiplet-assembly-based processors,” IEEE Transactions on Very\nLarge Scale Integration (VLSI) Systems , 2020.\n[23] A. Samajdar, J. M. Joseph, Y . Zhu, P. Whatmough, M. Mattina, and\nT. Krishna, “A systematic methodology for characterizing scalability of\ndnn accelerators using scale-sim,” in IEEE International Symposium on\nPerformance Analysis of Systems and Software (ISPASS) , 2020.\n[24] G. Shan, Y . Zheng, C. Xing, D. Chen, G. Li, and Y . Yang, “Architecture\nof computing system based on chiplet,” Micromachines , 2022.\n[25] Y . S. Shao, J. Clemons, R. Venkatesan, B. Zimmer, M. Fojtik, N. Jiang,\nB. Keller, A. Klinefelter, N. Pinckney, P. Raina et al. , “Simba: Scaling\ndeep-learning inference with multi-chip-module-based architecture,” in\n52nd IEEE/ACM International Symposium on Microarchitecture (MI-\nCRO) , 2019.\n[26] N. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E. Papalex-\nakis, and C. Faloutsos, “Tensor decomposition for signal processing and\nmachine learning,” IEEE Transactions on Signal Processing , 2017.\n[27] D. Stow, Y . Xie, T. Siddiqua, and G. H. Loh, “Cost-effective design of\nscalable high-performance systems using active and passive interposers,”\nin36th IEEE/ACM International Conference on Computer-Aided Design\n(ICCAD) , 2017.\n[28] Z. Tan, H. Cai, R. Dong, and K. Ma, “NN-baton: DNN workload orches-\ntration and chiplet granularity exploration for multichip accelerators,”\ninACM/IEEE 48th International Symposium on Computer Architecture\n(ISCA) , 2021.\n[29] T. Tang and Y . Xie, “Cost-aware exploration for chiplet-based architec-\nture with advanced packaging technologies,” arXiv:2206.07308 , 2022.\n[30] TensorNetwork, “Tensor Diagram Notation,” https://tensornetwork.org/\ndiagrams/.\n[31] UCIe, “UCIe white paper,” https://www.uciexpress.org/general-8.\n[32] M. Wang, Y . Wang, C. Liu, and L. Zhang, “Network-on-interposer\ndesign for agile neural-network processor chip customization,” in 58th\nACM/IEEE Design Automation Conference (DAC) , 2021.\n[33] X. Wei, Y . Liang, X. Li, C. H. Yu, P. Zhang, and J. Cong, “TGPA:\nTile-grained pipeline architecture for low latency cnn inference,” in\n37th IEEE/ACM International Conference on Computer-Aided Design\n(ICCAD) , 2018.\n[34] Y . N. Wu, J. S. Emer, and V . Sze, “Accelergy: An architecture-\nlevel energy estimation methodology for accelerator designs,” in 38th\nIEEE/ACM International Conference on Computer-Aided Design (IC-\nCAD) , 2019.\n[35] Q. Xiao, S. Zheng, B. Wu, P. Xu, X. Qian, and Y . Liang, “HASCO:\ntowards agile hardware and software co-design for tensor computation,”\nin48th ACM/IEEE International Symposium on Computer Architecture\n(ISCA) , 2021.\n[36] J. Yin, Z. Lin, O. Kayiran, M. Poremba, M. S. B. Altaf, N. E. Jerger, and\nG. H. Loh, “Modular routing design for chiplet-based systems,” in 45th\nACM/IEEE International Symposium on Computer Architecture (ISCA) ,\n2018.\n[37] X. Zhang, C. Hao, P. Zhou, A. Jones, and J. Hu, “H2H: heterogeneous\nmodel to heterogeneous system mapping with computation and commu-\nnication awareness,” in 59th ACM/IEEE Design Automation Conference\n(DAC) , 2022.\n[38] S. Zheng, X. Zhang, L. Liu, S. Wei, and S. Yin, “Atomic dataflow\nbased graph-level workload orchestration for scalable dnn accelerators,”\nin28th IEEE International Symposium on High-Performance Computer\nArchitecture (HPCA) , 2022.\n[39] S. Zheng, R. Chen, A. Wei, Y . Jin, Q. Han, L. Lu, B. Wu, X. Li,\nS. Yan, and Y . Liang, “Amos: enabling automatic mapping for tensor\ncomputations on spatial accelerators with hardware abstraction,” in 49th\nInternational Symposium on Computer Architecture (ISCA) , 2022.\n[40] S. Zheng, S. Chen, and Y . Liang, “Memory and computation coordinated\nmapping of dnns onto complex heterogeneous soc,” in 60th ACM/IEEE\nDesign Automation Conference (DAC) , 2023.\n9\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:48:06 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 9,
      "source": "uploads/20251211_195952_Monad_Towards_Cost-Effective_Specialization_for_Chiplet-Based_Spatial_Accelerators.pdf"
    }
  }
}