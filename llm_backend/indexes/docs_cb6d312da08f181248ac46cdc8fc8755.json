{
  "0": {
    "text": "Cost-Performance Co-Optimization for the Chiplet Era  \n \nAlexander Graening*, Darayus Adil Patel†, Giuliano Sisto†, Erwan Lenormand†, Manu Perumkunnil†, Nicolas Pantano†, \nVinay B.Y. Kumar†, Puneet Gupta*, Arindam Mallik† \n*University of California, Los Angeles, California, USA †imec, Kapeldreef 75, 3001 Leuven, Belgium  \nagraening@ucla.edu, darayus.adil.patel@imec.be, giuliano.sisto@imec.be, erwan.lenormand@imec.be, \nmanu.perumkunnil@imec.be, vinay.kumar.b aapanapalliyadaiah@imec.be, puneet@ ee.ucla.edu, arindam.mallik@imec.be \n \nAbstract \nChiplet technologies allow for greater flexibility in system \ndesign through a wide range of system configuration options \nspanning integration schemes (monolithic, 2.5D, 3D), \nheterogeneity in technology node s, and partitioning of system \nresources. Each such configuration has implications on figures of merit such as cost, power and performance. Optimizing for one metric may come at the expense of the other two. During \nthe initial architecture exploration and design planning stage, \nit is critical to conduct cost -performance co -optimization \nusing models that cover the entire spectrum of configuration options to make an informed choice.  This work presents an \nevaluation using a framework that models both cost and \nperformance simultaneously for chiplet-based systems , \nenabling analysis of the impact of various  system-wide \narchitectural configurations. We analyze a representative \nscale-out system for high-performance computing workloads to investigate how factors like integration, partitioning, technology node choices, and system size affect power, performance, and cost. The results demonstrate that the \noptimal design choices vary depending on these factors, \nhighlighting the insights early stage chiplet design space exploration can offer.  \nIntroduction \nAdvanced integration and packaging will drive the scaling \nof computing systems in the ne xt decade. Chip let systems [1, \n2] are becoming increasingly prevalent in the industry for high \nperformance computing and are under active exploration for \nautomotive systems [3 ]. NVIDIA [4], Intel [5], and AMD [6] \nhave all released chiplet-based products.  The diversity of \nintegration choices (monolithic, 2.5D, 3D) [2] and partitioned \ndie sizes for chiplet based systems enlarges the design space \nand widens the attainable cost and performance profile. This \nnecessitates cost-performance co-optimization to determine \nideal system configuration [7]. \nWith respect to performance, chiplets allow for integration \nof very large systems. Instead of integrating these systems as \nmultiple packaged chips, we can integrate them as a chiplet \nsystem with near monolithic performance. While integrating \nlarge systems in a single package tends to improve \nperformance, disaggregating monolithic designs into multiple \nchiplets can hurt performance due to the increased signal \nlengths resulting from die separation. Generally splitting a design and integrating in a 2D or 2.5D manner will result in worse or equivalent performance, but in the case of 3D \nstacking there can be a performance improvement over the \nmonolithic case due to reducing signal length by changing \nlong cross-die connections into relatively short vertical \nconnections. Chiplet-based design can offer significant reduction in \nsystem cost due to improving yield, however, there are other \nconfounding factors to consider.  A chiplet design requires more engineering work upfront to appropriately partition the system into the optimal number and kinds of chiplets. Chiplet \nsystems are more expensive to  assemble and package than \nmonolithic designs, particularly if complicated stacking or \ninterposers are required. Additionally, splitting a design into \nchiplets requires adding die-to-die interface IOs that increase \ntotal area and power for the design. \nChiplets also have other benefits that are more difficult to \nquantify. Creating a new design based on previously designed \nchiplets can be fast er than creating the design from scratch, \nreducing time to market. Thus, reuse is an important design \nconsideration. Additionally, if it is easier to customize \ndifferent product instances with minimal silicon waste, it can be practical to offer more variations of a product family by \noffering different combinations of chiplets with minimal re-design. \nIn addition to the inherent benefits and drawbacks of \ndifferent chiplet arch itecture options, the cost and performance \nof chiplet systems depends on factors such as inter-chiplet IO \nscheme, substrate type, and bump pitch. These factors are \nimportant to consider early in the design process. This requires \ndetailed cost and performance modeling to make informed design decisions. \n In this work, we motivate the need for both cost and \nperformance modeling during the architectural definition \nphase to identify cost-performance co-optimal points in the design space. For a representative scale-out system designed for HPC workloads, we study the impact of partitioning, \nintegration choices, technology node, and system size on \nindividual and combined metrics involving performance, \npower, and cost. Our results illustrate the variation in co-\noptimal design points, highlighting the insights that \nperformance and cost modeling for early stage chiplet design space can offer. \nTher rest of the paper is organized as follows. First, we \ndiscuss related works, then we describe the example system \nwe used for our study. Next, we describe our modeling framework with details about both our cost model and our \nperformance model. After this we show the results of our \nsystem analysis on a variety of metrics.  \nRelated Work \nThe manufacturing cost of dies and silicon interposers has \nbeen previously studied in [8] and [9]. However, these studies \ndo not account for the cost for substrates, die-to-die (D2D) \noverhead, or non-recurring engineering (NRE) costs. \nQuantitative cost modeling for chiplet-based designs has been \n40 979-8-3315-2200-1/24/$31.00 ©2024 IEEE2024 IEEE 26th Electronics Packaging Technology Conference (EPTC) | 979-8-3315-2200-1/24/$31.00 ©2024 IEEE | DOI: 10.1109/EPTC62800.2024.10909776\nAuthorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:37:29 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 1,
      "source": "uploads/20251211_194938_Cost-Performance_Co-Optimization_for_the_Chiplet_Era.pdf"
    }
  },
  "1": {
    "text": "published in “Chiplet Actuary” [9] and “Chiplets: How Small \nis too Small” [10]. These analytical cost models account for \nmultiple elements of chiplet cost including manufacturing \nyield, packaging cost, IO requirements, technology nodes, and many other factors. In particular, the work in [10] provides an \nanalytical framework to dete rmine the system size above \nwhich disaggregating an SoC into smaller chiplets yields cost benefits and below which disaggregating will tend to carry a cost penalty. \nThe total cost of ownership (TCO) benefit of employing \nchiplet-based systems in building AI supercomputers for \nserving large language model (LLM) workloads has been established in the work on Chiplet Cloud [11] where the authors demonstrate that chip costs dominate TCO. The study \nin [12] explores the question of how to partition compute \nresources across chiplets as  well as the trade-off in \nperformance versus cost and sustainability. The impact of inter-chiplet network in 2.5D integration on system \nperformance for deep learning inference workloads has been studied in [13]. A design space exploration framework for chiplet based processors is shown in [14], while [15] proposes \nHISIM, a benchmarking tool for chiplet-based heterogeneous \nintegration that evaluates the performance of monolithic, 2.5D and 3D systems. \nIt is pertinent to note that none of the existing works in \nliterature simultaneously explore the performance and cost variation for chiplet-based sy stems wherein the integration \nscheme, technology node, system partitioning, and system size are simultaneously varied. \nSystem Description \nFig. 1 illustrates components of a single node of a scale-\nout system architecture. We consider a single node of this system for cost-performance evaluation, and it is composed of: \nx Compute arrays containing control cores, RISC-V \nbased SIMT compute cores (CCs), and local SRAM. \nx Data processing unit (DPU) attached to a DRAM \n(HBM) and a storage system. \nx Network processing unit (NPU) building a distributed \nstorage system. \nNodes are integrated into bo ards and the full system is \ncomposed of a network of boards. Cost and performance are \nintertwined and impacted by the system architecture and \nphysical SoC configuration. We examine a few physical \nconfiguration options (Fig. 2) to evaluate the trade-offs therein. Cost is estimated for a single package and components \nthat are shared between p ackages and thus packaged \nseparately – i.e., the colored blocks in Fig. 1 – are ignored \nsince they do not change across the chosen configurations. \nThe rest of the logic and memory silicon (including HBM) is split up according to the configurations shown in Fig. 2. \nThe four chiplet-configuration options shown in Fig. 2 \ncontain the same HBM chiplet and have different \nconfigurations for the compute cores and memory. In the first column, labeled “fewer chiple ts,” everything except the HBM \nis integrated in a single die/stack. This consists of a single \nmonolithic chiplet and a 3D stack of logic on memory respectively. The “more chiple ts” column contains both a \nmore aggressively split 2.5D integration option and a version including 3D stacking. In both cases, the DPU is placed as its \nown independent chiplet while the memory and cores are split \ninto two groups either integrated as 2 separate chiplets or as \ntwo separate 3D stacks. \nWe evaluated these configurations for both 7nm and 5nm. \nAdditionally, we considered a variety of system sizes by varying the number of compute cores per board and boards per system. The area measures we used for this study are from \nphysical aware synthesis. \n \n \nFig. 1.  System architecture under evaluation consists of \ncompute tiles, data processing unit (DPU), HBM, network \nprocessing unit (NPU), and storage (SSD). \n \n \nFig. 2.  Configurations for cost-performance co-optimization. \n2.5D splits SoC into 3 chiplets and S3D splits L3D stack into \ntwo 3D stacks plus one additional chiplet. \n \n \nFig. 3.  Evaluation framework. \n2024 IEEE 26th Electronics Packaging Technology Conference (EPTC) 41Authorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:37:29 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 2,
      "source": "uploads/20251211_194938_Cost-Performance_Co-Optimization_for_the_Chiplet_Era.pdf"
    }
  },
  "2": {
    "text": "System Modeling \nFig. 3 depicts our evaluation framework that \nsimultaneously models cost and performance for chiplet-based \nsystems used for analyzing the impact of different system-\nwide architectural configurations, integration scheme, \ntechnology node, system par titioning, system size and \nworkloads. The framework supports modeling and evaluating \na configurable high-performance computing (HPC) system \ncomprised of compute core/s, memory and network components. The inherent cost and performance models \nembedded in our framework are detailed below.  \nCost Model \nTo analyze system cost, we us ed the open source chiplet \ncost model previously described in [10]. This cost model uses \na nested stack of chip class objects meant to allow flexible \ncost analysis of arbitrary chiplet based designs. The model is fully parameterized so users can define custom technologies \nand processes for their studies. This model is open source at https://github.com/nanocad-lab/cost_model_chiplets . \nFig. 4 depicts the required input parameters for the cost \nmodel. Bonding pitch, assembly machine costs, assembly \nyields and other parameters are included in the assembly \ntechnology process. Assembly cost and yield are modeled to \nbe dependent on the nature of bonding (e.g., solder reflow vs. \ndie-to-wafer thermocompression bonding) and bump pitches \nwhich influence tool cost. The IO cell includes the additional area required to drive a signal over a longer distance between chiplets compared to a monolithic design as well as the \nadditional energy per bit. IOs are only placed in regions which \nmeet the reach requirement for the cell type. Total chiplet area is calculated as the larger of e ither core area plus IO cell area \nor the area required by bumps for signaling and power. The \nlayer definition contains parameters such as cost per mm\n2 and \ndefect density required to compute the cost and yield of dies. \nYield is computed using the negative binomial yield model \n[16]. The wafer process contai ns reticle size and dicing \ninformation to improve the accuracy of the cost per die. We \nassume pre-bond known-good-die testing and ignore test cost. \nOur cost model currently does not incorporate IP, license, and \nboard costs. \n \n \nFig. 4.  Cost model overview. The model provides a wide \nrange of parameters that influence cost. \n \nThe negative binomial yield model used in this work uses \nparameters representative of advanced CMOS nodes. Most \nparameters are scaled down fro m their correspondent value for \nthe 10nm node. For all options, the portion of area considered \ncritical is 60%, the clustering factor parameter in the yield \nmodel is assumed to be equal to 2 and the stitching yield as 0.5. A few of the parameters we used for yield and cost \ncalculations that vary by process technology node are shown \nin Table 1. In Fig. 5, we show the portion of cost for each \nconfiguration that comes from the costs and yield impacts of \nthe chiplet configuration. Cost of assembly and packaging configuration are important to consider for chiplet systems. \n \nTable 1.  Parameter values used  to differentiate between 5nm \nand 7nm cost and yield for the chiplet-based cost model. \n \n \n \nFig. 5.  Package cost breakdown (%). Bonding/packaging is \nfor yield plus material cost not including substrate. \n \n \nFig. 6.  Substrate cost trends for each configuration. \n Organic substrates are typically cheaper than silicon, but at \nthe expense of having much larger bump pitch. We looked at an organic substrate with chiplets bonded simultaneously \nusing a reflow style bonding and compared this to a silicon \nsubstrate with a much lower pitch using thermal compression bonding where each die is placed and bonded individually. We \n42 2024 IEEE 26th Electronics Packaging Technology Conference (EPTC)Authorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:37:29 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 3,
      "source": "uploads/20251211_194938_Cost-Performance_Co-Optimization_for_the_Chiplet_Era.pdf"
    }
  },
  "3": {
    "text": "compared switching from the 110um pitch organic substrate to \nthe 10um pitch silicon substrate in Fig. 6 and Fig. 7. The \nsilicon substrate cases are more expensive than organic. The \ncost of the silicon substrate is higher than the almost negligible cost of the organic substrate, but this is the primary \ndifference between the cases. Fig. 6 shows that this case study \nis not pitch-bound as the cost of the organic substrate case \nremains the same for a smaller bump pitch and the cost of the \nsilicon substrate case remains the same for a larger bump \npitch. Silicon substrates with small bump pitch make more \nsense in cases that are pitch limited with a large number of \nIOs. \n \n \nFig. 7.  Cost with bump pitches and substrates for 2.5D case. \n \nPerformance Model \nA detailed, functionally correct and cycle approximate \nexecutable model of the system is used for cost-performance \ncodesign (Fig. 3) and studied with representative workloads. \nThe performance model has relevant parameter sensitivity to \naccount for different chiplet configuration options (Fig. 2) i.e. \nlatency and bandwidth between logic/memory in 2.5D vs. 3D \nand with D2D interfaces over UCIe. This work uses large sparse matrix-vector multiplication workloads distributed \nacross the entire system measuring Giga floating-point \noperations per second (GFLOPS/s) in steady state as a \nperformance metric. The system with multiple interconnected \nboards is simulated using a parallel discrete event simulator. \n It may be noted that the cost and performance models are \ndecoupled, and any other system performance model whose \nperformance estimates can be sensitive to such configuration \nparameters can also be used. For instance, our model uses a \n3D SRAM performance model for 3D configuration \n(accounting for latency and bandwidth improvements) vs. \nnon-3D configurations. \nSystem-level power estimation is done as a function of \nsystem parameters and the underlying components. This uses \npower estimates via characteriza tion of individual function \nblocks within the components with EDA tools targeting \nrelevant technology nodes. Additionally, variation in IO \npower as a function of system partitioning (Fig. 2) is also \naccounted for in system-level power. Hierarchical area \nestimation also follows  a similar approach. Results and Discussion \nWhile the results and observations here correspond to a \nspecific system architecture, our conclusions and methodology can be helpful for informing ot her chiplet system architecture \nconfiguration choices. \nPerformance measurements in Figs. 8-13 correspond to \nexecuting large sparse matrix-vector multiplication \n(SPMV) distributed across various parameterized \nconfigurations of the system.  In these figures, a tile is a unit \nof design containing several cores within a package (Fig. 1). \nMore tiles mean a larger package which impacts package cost. \nFig. 8 shows the normalized system performance for the \ndifferent system configurations and package sizes. Fig. 9-13 consider 5nm and 7nm variants with an iso-performance \nassumption. It is also interesting to note that the performance \nbenefit of 3D stacking in the L3D case is mostly lost in the \nS3D case compared to  the SoC case due to the increased \nsignal distance in inter- chiplet communication. \nFig. 9 shows normalized system power. Since we assume \niso-performance for 7nm and 5nm, we do see a power benefit for 5nm. We do not observe any appreciable power difference between the chiplet configurations, but the options with more \nchiplets (2.5D and S3D) and so more interconnect power have \nslightly higher power than the options with fewer chiplets (SoC and L3D), matching our expectations. Note that power scales better for 5nm than for 7nm.\n \nFig. 10 shows the normalized system cost. Here we see the \nimpact of the system size on the preferred number of chiplets. \nThe L3D case (2 stacked  chiplets + HBM) is cheapest for the \nsmallest 2-tile system while the S3D (5 total chiplets + HBM) \nis better for the larger 8-tile system. We also see the SoC (1 chiplet + HBM) is already impacted by yield in the 2-tile case and is consistently the worst option for cost.\n \nIn a power-constrained application, we care about how \nmany operations can be executed  on a power budget. Fig. 11 \nshows the performance per Watt for our system. Note that \nL3D consistently outperforms SoC and S3D consistently outperforms 2.5D in the same technology due to the performance benefit of 3D stac king memory on compute. We \nalso see all 5nm options are better than all 7nm options for the \n8-tile case due to the spike in power we see in Fig. 9. \n \n \nFig. 8.  Normalized system performance.  \n \n2024 IEEE 26th Electronics Packaging Technology Conference (EPTC) 43Authorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:37:29 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 4,
      "source": "uploads/20251211_194938_Cost-Performance_Co-Optimization_for_the_Chiplet_Era.pdf"
    }
  },
  "4": {
    "text": " \nFig. 9.  Normalized system power. \n \n \nFig. 10.  Normalized system cost \n \nIf our application is cost constrained rather than power, we \ncare about maximizing the operatio ns we can get for a certain \nmonetary budget. Fig. 12 shows the impact on performance \nper dollar. Here we see the 3D stacked cases behaving well due to their increased performance and we see a swap between the higher performing L3D case and the cheaper S3D case for the 8-tile package due to the much better yield in the S3D \ncase. It is also worth noting that 5nm does not compare well to \n7nm in this study since it is more expensive, and we assumed \niso-performance. \nIf we want to optimize for all 3 metrics, then we can look \nat the performance per Watt per dollar. Fig. 13 shows that L3D is the best in terms of this metric for 2-tile and 4-tile \nsystems, but S3D in 7nm is the best for the 8-tile system. This \nis largely the result of the performance per dollar we saw in Fig. 12, but it is interesting to note how the gap has closed between 5nm and 7nm with the inclusion of power. Again, we see 5nm get a noticeable edge due  to the spike in power for \n7nm in the 8-tile case shown in Fig. 9. \nThese measurements show ho w co-design with a target \nworkload can lead to different conclusions dependent on size of system and choice of optimization target among cost, power, and performance.\n \n \nFig. 11.  Normalized performance per Watt. \n \n \n \nFig. 12.  Normalized performance per dollar. \n \n \n \nFig. 13.  Normalized performance per Watt per dollar. \n   \n44 2024 IEEE 26th Electronics Packaging Technology Conference (EPTC)Authorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:37:29 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 5,
      "source": "uploads/20251211_194938_Cost-Performance_Co-Optimization_for_the_Chiplet_Era.pdf"
    }
  },
  "5": {
    "text": "Conclusions \nEarly cost, power, and performance co-optimization for \nchiplet systems is critical since changing the chiplet configuration gets more expensive later into design. Detailed \nsystem modeling can be used to allow an informed choice \nearly in the design process. We provide insights on how doing this for a realistic system can steer 2.5D and 3D design decisions. Our cost model is open source and can be used together with any other detailed system performance model. \nOur results allow us to draw several conclusions:\n \nƔ Advanced packaging (silicon, small bump pitch) is \nnot necessary in our system, but might become \nnecessary if the in ter-chiplet bandwidth or number of \nHBMs increases. \nƔ Normalized package cost and chipletization benefit \nincrease with system size due to yield. \nƔ Small systems where chiplet yield is less of a driving \nfactor favor fewer chiplets  due to assembly cost. \nƔ 3D stacking improves performance over monolithic, \nbut performance equalizes if we split to multiple 3D stacks to improve cost and yield. \nWhile our co-optimal design po ints are specific to system \narchitecture, technology, and workload dynamics, the insights of our study highlight the benefits that performance and cost \nmodeling for early stage chiplet design space exploration can offer. \nAcknowledgments \nThis work is supported, in part, by NSF and DARPA/SRC \nCHIMES JUMP 2.0 Center. \nReferences \n1. Chiplets: Piecing Together the Next Generation of Chips \nhttps://www.imec-int.com/en/articles/chiplets-piecing-\ntogether-next-generation-chips-part-i \n2. Li, T.; Hou, J.; Yan, J.; Liu, R.; Yang, H.; Sun, Z. C hiplet \nHeterogeneous Integration Technology—Status and \nChallenges. Electronics  2020, 9, 670. https://doi.org/10.33 \n90/electronics9040670  \n3. “Automotive chiplet program,” imec. Accessed: Sep. 15, \n2024. [Online]. Available: https://www.imec-int.com/en/ \nexpertise/cmos-advanced-and-beyond/compute/automotive \n-chiplet-program  \n4. TechPowerUp. NVIDIA H100 SXM5 96 GB. \nTechpowerup.co m https://www.techpowerup.com/gpu-\nspecs/h100-sxm5-96-gb.c3974 (2024). \n5. “Intel Data Center GPU Max Series Overview,” Intel. \nAccessed: Sep. 13, 2024. [Online]. Available: \nhttps://www.intel.com/content/www/us/en/developer/articl\nes/technical/intel-data-center-gpu-max-series-overview.html \n6. S. Naffziger et al ., \"Pioneering Chiplet Technology and \nDesign for the AMD EPYC™ and Ryzen™ Processor \nFamilies : Industrial Product,\" 2021 ACM/IEEE 48th \nAnnual International Symposium on Computer \nArchitecture (ISCA) , Valencia, Spain, 2021, pp. 57-70, \ndoi: 10.1109/ISCA52012.2021.00014.   \n7. Pal, S., Mallik, A. & Gupta, P. System technology co-\noptimization for advanced integration. Nat Rev Electr Eng  \n1, 569–580 (2024). https://doi.org/10.1038/s44287-024-\n00078-x 8. Dylan Stow, Itir Akgun, Russell Barnes, Peng Gu, and \nYuan Xie. 2016. Cost analysis and cost-driven IP reuse \nmethodology for SoC design based on 2.5D/3D \nintegration. In ICCAD.  \n9. Yinxiao Feng and Kaisheng Ma. 2022. Chiplet actuary: a \nquantitative cost model and multi-chiplet architecture \nexploration. In Proceedings of the 59th ACM/IEEE Design Automation Conference (DAC '22). Association for Computing Machinery, New York, NY, USA, 121–126. https://doi.org/10.1145/3489517.3530428 \n10. A. Graening, S. Pal and P. Gupta, \"Chiplets: How Small is \ntoo Small?,\" 2023 60th ACM/IEEE Design Automation \nConference (DAC), San Francisco, CA, USA, 2023, pp. 1-6, doi: 10.1109/DAC56929.2023.10247947. \n11. Peng, Huwan, et al. \"Chiplet cloud: Building ai \nsupercomputers for serving large generative language models.\" arXiv preprint arXiv:2307.02666  (2023).  \n12. Zhang, Shiqing, et al. \"Balancing performance against cost \nand sustainability in multi-chip-module GPUs.\" IEEE \nComputer Architecture Letters  (2023).  \n13. Sharma, Harsh, et al. \"A chieving datacenter-scale \nperformance through chiplet-based manycore architectures.\" 2023 Design, Automation & Test in Europe \nConference & Exhibition (DATE) . IEEE, 2023.  \n14. S. Pal, D. Petrisko, R. Kumar and P. Gupta, \"Design Space \nExploration for Chiplet-Assembly-Based Processors,\" in \nIEEE Transactions on Very Large Scale Integration (VLSI) Systems , vol. 28, no. 4, pp. 1062-1073, April 2020, \ndoi: 10.1109/TVLSI.2020.2968904.  \n15. Z. Wang et al., \"Benchmarking Heterogeneous Integration \nwith 2.5D/3D Interconnect Modeling,\" 2023 IEEE 15th \nInternational Conference on ASIC (ASICON) , Nanjing, \nChina, 2023, pp. 1-4, doi: 10.1109/ASICON58565.2023.1 \n0396377.  \n16. W. Kuo and T. Kim, \"An overview of manufacturing yield \nand reliability modeling for semiconductor products,\"  \nProceedings of the IEEE , vol. 87, no. 8, pp. 1329-1344, \nAug. 1999, doi: 10.1109/5.775417. \n2024 IEEE 26th Electronics Packaging Technology Conference (EPTC) 45Authorized licensed use limited to: INSTITUTE OF COMPUTING TECHNOLOGY CAS. Downloaded on August 04,2025 at 09:37:29 UTC from IEEE Xplore.  Restrictions apply. ",
    "metadata": {
      "page": 6,
      "source": "uploads/20251211_194938_Cost-Performance_Co-Optimization_for_the_Chiplet_Era.pdf"
    }
  }
}