from typing import Dict, List

# ---- åº”ç”¨å†…è‡ªå®šä¹‰æ¨¡å—ï¼ˆé¡¹ç›®å†…éƒ¨å¯¼å…¥ï¼‰----
from app.services.llm_factory import LLMFactory
from app.services.search_service import SearchService
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
# ---- é™æ€æ–‡ä»¶æ”¯æŒ ----
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

 # å…è®¸æµè§ˆå™¨å‰ç«¯ï¼ˆå¦‚Vue/Reactï¼‰ä»ä¸åŒåŸŸè®¿é—®ä½ çš„APIã€‚


app = FastAPI(title="AssistGen REST API")



# ==============================================
# ğŸŒ CORS è®¾ç½®ï¼ˆè·¨åŸŸè®¿é—®ï¼‰
# ==============================================
app.add_middleware(
    CORSMiddleware,              # ä½¿ç”¨ FastAPI æä¾›çš„è·¨åŸŸä¸­é—´ä»¶
    allow_origins=["*"],         # å…è®¸æ‰€æœ‰åŸŸè®¿é—®ï¼ˆå¼€å‘æ—¶å¯ç”¨ï¼Œç”Ÿäº§è¦æ”¹ä¸ºå…·ä½“åŸŸåï¼‰
    allow_credentials=True,      # æ˜¯å¦å…è®¸è·¨åŸŸæºå¸¦ cookie
    allow_methods=["*"],         # å…è®¸æ‰€æœ‰ HTTP æ–¹æ³•ï¼šGETã€POSTã€PUTã€DELETE ç­‰
    allow_headers=["*"],         # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)




class ReasonRequest(BaseModel):
    # messages: ä¸€ç»„æ¶ˆæ¯ï¼Œæ¯æ¡æ˜¯ {"role": "user", "content": "é—®é¢˜å†…å®¹"} è¿™ç§å½¢å¼
    messages: List[Dict[str, str]]

class ChatMessage(BaseModel):
    # å®šä¹‰â€œèŠå¤©æ¥å£â€çš„è¯·æ±‚ä½“æ ¼å¼
    messages: List[Dict[str, str]]



@app.post("/chat")
async def chat_endpoint(request: ChatMessage):
    """
    æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯ï¼Œè°ƒç”¨ LLMFactory åˆ›å»ºçš„èŠå¤©æœåŠ¡ï¼Œå¹¶ä»¥æµå¼å½¢å¼è¿”å›ã€‚
    """
    try:
        # åˆ›å»ºèŠå¤©æœåŠ¡å®ä¾‹ï¼ˆç”± LLMFactory åŠ¨æ€å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹ï¼‰
        chat_service = LLMFactory.create_chat_service()

        # è¿”å› StreamingResponseï¼šè®©å‰ç«¯å¯ä»¥ä¸€è¾¹æ¥æ”¶ä¸€è¾¹æ˜¾ç¤ºæ¨¡å‹è¾“å‡º
        return StreamingResponse(
            chat_service.generate_stream(request.messages), 
            media_type="text/event-stream"  # æŒ‡å®šä¸º SSEï¼ˆServer-Sent Eventsï¼‰æ ¼å¼
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



@app.post("/reason")
async def reason_endpoint(request: ReasonRequest):
    """
    ç”¨äºé€»è¾‘æ¨ç†ç±»ä»»åŠ¡ï¼Œè°ƒç”¨ LLM çš„â€œæ¨ç†æœåŠ¡â€ã€‚
    """
    try:
        reasoner = LLMFactory.create_reasoner_service()
        return StreamingResponse(
            reasoner.generate_stream(request.messages),
            media_type="text/event-stream"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/search")
async def search_endpoint(request: ChatMessage):
    try:
        search_service = SearchService()
        return StreamingResponse(
            search_service.generate_stream(request.messages[0]["content"]),
            media_type="text/event-stream"
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£ï¼ˆGET /healthï¼‰
    ä¾›å¤–éƒ¨ç›‘æ§æˆ–K8sæ¢é’ˆä½¿ç”¨ï¼Œç”¨æ¥æ£€æµ‹æœåŠ¡æ˜¯å¦å­˜æ´»ã€‚
    """
    return {"status": "ok"}


# ==============================================
# ğŸ§± é™æ€æ–‡ä»¶æŒ‚è½½ï¼ˆå‰ç«¯èµ„æºï¼‰
# ==============================================
#app.mount("/", StaticFiles(directory="static/dist", html=True), name="static")
# å°†å‰ç«¯æ‰“åŒ…åçš„é™æ€æ–‡ä»¶ï¼ˆå¦‚Vue/Reactçš„distç›®å½•ï¼‰æŒ‚è½½åˆ°æ ¹è·¯å¾„â€œ/â€
# è®¿é—®æ ¹URLæ—¶ï¼Œä¼šç›´æ¥è¿”å› index.htmlï¼Œå®ç°å‰åç«¯æ•´åˆéƒ¨ç½²ã€‚
